{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "[Named Enity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is one of the most common [NLP](https://en.wikipedia.org/wiki/Natural-language_processing) problems. The goal is classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "*What can you use it for?* Here are a few ideas - social media, chatbot, customer support tickets, survey responses, and data mining! \n",
    "\n",
    "### Predicting named entities of GMB(Groningen Meaning Bank) corpus\n",
    "\n",
    "In this notebook we will perform a Sequence Tagging with a Bi-directional LSTM Neural Network to extract the named entities from the annotated corpus.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/named-entity-recognition-template/master/images/ner-image.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "Entity tags are encoded using a BIO annotation scheme, where each entity label is prefixed with either B or I letter. B- denotes the beginning and I- inside of an entity. The prefixes are used to detect multiword entities, e.g. sentence:\"World War II\", tags:(B-eve, I-eve, I-eve). All other words, which don‚Äôt refer to entities of interest, are labeled with the O tag.\n",
    "\n",
    "Tag | Label meaning | Example Given\n",
    "------------ | ------------- | \n",
    "geo | Geographical Entity | London\n",
    "org | Organization | ONU\n",
    "per | Person | Bush\n",
    "gpe | Geopolitical Entity | British\n",
    "tim | Time indicator | Wednesday\n",
    "art | Artifact | Chrysler\n",
    "eve | Event | Christmas\n",
    "nat | Natural Phenomenon | Hurricane\n",
    "O | No-Label | the\n",
    "\n",
    "We will:\n",
    "- Preprocess text data for NLP\n",
    "- Build and train a Bi-directional LSTM model using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "- Run the model on your own sentences!\n",
    "\n",
    "#### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome named entity recognition task.*\n",
    "\n",
    "Now, let's get started! üöÄ\n",
    "\n",
    "### Initial Setup\n",
    "Let's start by importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.3.1\n",
      "GPU detected: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"GPU detected:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-26f59eebcea8>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 5  # Number of passes through entire dataset\n",
    "    MAX_LEN = 50  # Max length of review (in words)\n",
    "    EMBEDDING = 40  # Dimension of word embedding vector\n",
    "\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 5\n",
    "    MAX_LEN = 50\n",
    "    EMBEDDING = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The movie ner dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  47960\n",
      "Number of words in the dataset:  35175\n",
      "Tags: ['L-art', 'I-nat', 'B-per', 'I-eve', 'L-nat', 'Tag', 'B-tim', 'U-per', 'I-tim', 'L-gpe', 'U-org', 'L-org', 'L-tim', 'U-geo', 'B-nat', 'I-gpe', 'B-eve', 'B-geo', 'L-eve', 'I-geo', 'U-eve', 'U-gpe', 'L-per', 'U-nat', 'U-art', 'B-org', 'B-gpe', 'I-art', 'I-org', 'L-geo', 'I-per', 'U-tim', 'O', 'B-art']\n",
      "Number of Labels:  34\n",
      "What the dataset looks like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence #</td>\n",
       "      <td>Word</td>\n",
       "      <td>Tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>London</td>\n",
       "      <td>U-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence:0</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word    Tag\n",
       "0  Sentence #           Word    Tag\n",
       "1  Sentence:0      Thousands      O\n",
       "2  Sentence:0             of      O\n",
       "3  Sentence:0  demonstrators      O\n",
       "4  Sentence:0           have      O\n",
       "5  Sentence:0        marched      O\n",
       "6  Sentence:0        through      O\n",
       "7  Sentence:0         London  U-geo\n",
       "8  Sentence:0             to      O\n",
       "9  Sentence:0        protest      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/biluo_ner_dataset.csv\", encoding=\"latin1\", names=['Sentence #', 'Word', 'Tag'])\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "print(\"What the dataset looks like:\")\n",
    "# Show the first 10 rows\n",
    "data.head(n=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what a sentence looks like:\n",
      "[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'U-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'U-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'U-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \"\"\"Class to Get the sentence in this format:\n",
    "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Args:\n",
    "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        \"\"\"Return one sentence\"\"\"\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "print('This is what a sentence looks like:')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output Cell above, each sentence in the dataset is represented as a list of tuple: [`(Token_1, PoS_1, Tag_1)`, ..., `(Token_n, PoS_n, Tag_n)`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all the sentences\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Plot sentence by lenght\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before feeding the data into the model, we have to preprocess the text.\n",
    "\n",
    "- We will use the `word2idx` dictionary to convert each word to a corresponding integer ID and the `tag2idx` to do the same for the labels. Representing words as integers saves a lot of memory!\n",
    "- In order to feed the text into our Bi-LSTM-CRF, all texts should be the same length. We ensure this using the `sequence.pad_sequences()` method and `MAX_LEN` variable. All texts longer than `MAX_LEN` are truncated and shorter texts are padded to get them to the same length.\n",
    "\n",
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Sample:  Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Raw Label:  O O O O O O U-geo O O O O O U-geo O O O O O U-gpe O O O O O\n",
      "After processing, sample: [13700 28984  5457 32927 15983 25605 28503  2054 25725 20618 24774 16157\n",
      "  5195  5456 21033 20618 33261 28984  7946 21059 21882  2466 27301 31550\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "After processing, labels: [[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Key:word -> Value:token_index\n",
    "# The first 2 entries are reserved for PAD and UNK\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "#print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
    "#print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n",
    "\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
    "\n",
    "print('Raw Sample: ', ' '.join([w[0] for w in sentences[1]]))\n",
    "print('Raw Label: ', ' '.join([w[1] for w in sentences[1]]))\n",
    "print('After processing, sample:', X[1])\n",
    "print('After processing, labels:', y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will implement a Bidirectional Lstm Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 100)           3517700   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 35)            7035      \n",
      "=================================================================\n",
      "Total params: 3,685,535\n",
      "Trainable params: 3,685,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_word = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, input_length=MAX_LEN)(input_word)\n",
    "model = SpatialDropout1D(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags+1, activation='softmax'))(model)\n",
    "model = Model(input_word, out)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluate\n",
    "\n",
    "The Training is defined at the beginning by the type of instance on which runs:\n",
    "\n",
    "- On CPU machine: 25 minutes for 5 epochs.\n",
    "- On GPU machine: 5 minute for 5 epochs.\n",
    "\n",
    "*Note*: Accuracy isn't the best metric to choose for evaluating this type of task because most of the time it will correctly predict '**O**' or '**PAD**' without identifing the important Tags, which are the ones we are interested in. So after training for some epochs, we can monitor the **precision**, **recall** and **f1-score** for each of the Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1214/1214 [==============================] - 130s 107ms/step - loss: 0.2107 - accuracy: 0.9526 - val_loss: 0.0691 - val_accuracy: 0.9808\n",
      "Epoch 2/5\n",
      "1214/1214 [==============================] - 134s 110ms/step - loss: 0.0509 - accuracy: 0.9852 - val_loss: 0.0530 - val_accuracy: 0.9846\n",
      "Epoch 3/5\n",
      "1214/1214 [==============================] - 134s 111ms/step - loss: 0.0372 - accuracy: 0.9887 - val_loss: 0.0509 - val_accuracy: 0.9848\n",
      "Epoch 4/5\n",
      "1214/1214 [==============================] - 134s 110ms/step - loss: 0.0309 - accuracy: 0.9902 - val_loss: 0.0517 - val_accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "1214/1214 [==============================] - 130s 107ms/step - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.0547 - val_accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 3s 22ms/step - loss: 0.0528 - accuracy: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05280356854200363, 0.9853377938270569]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_te, np.array(y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate some samples in the test set. (At each execution it will test on a different sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number 1802 of 4796 (Test Set)\n",
      "Word           ||True ||Pred\n",
      "==============================\n",
      "A              : O     O\n",
      "White          : B-org B-org\n",
      "House          : L-org L-org\n",
      "spokesman      : O     O\n",
      ",              : O     O\n",
      "in             : O     O\n",
      "a              : O     O\n",
      "statement      : O     O\n",
      ",              : O     O\n",
      "said           : O     O\n",
      "the            : O     O\n",
      "referendum     : O     O\n",
      "represents     : O     O\n",
      "an             : O     O\n",
      "important      : O     O\n",
      "step           : O     O\n",
      "in             : O     O\n",
      "the            : O     O\n",
      "democratic     : O     O\n",
      "process        : O     O\n",
      "in             : O     O\n",
      "the            : O     O\n",
      "Democratic     : B-geo B-geo\n",
      "Republic       : L-geo I-geo\n",
      "of             : O     I-geo\n",
      "Congo          : U-geo L-geo\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0,X_te.shape[0]) # choose a random number between 0 and len(X_te)\n",
    "p = model.predict(np.array([X_te[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_te[i], -1)\n",
    "\n",
    "print(\"Sample number {} of {} (Test Set)\".format(i, X_te.shape[0]))\n",
    "# Visualization\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_te[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving Vocab\n",
    "with open('../models/word_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "with open('../models/index_to_tag.pickle', 'wb') as handle:\n",
    "    pickle.dump(idx2tag, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "model.save('../models/bilstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving Vocab\n",
    "with open('../models/word_to_index.pickle', 'rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    " \n",
    "# Saving Vocab\n",
    "with open('../models/index_to_tag.pickle', 'rb') as handle:\n",
    "    idx2tag = pickle.load(handle)\n",
    "    \n",
    "# Saving Model Weight\n",
    "model = load_model('../models/bilstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn\n",
    "\n",
    "Test out the model you just trained. Run the code Cell below and type your reviews in the widget, Have fun!üéâ\n",
    "\n",
    "Here are some inspirations:\n",
    "\n",
    "- Obama was the president of USA.\n",
    "- The 1906 San Francisco earthquake was the biggest earthquake that has ever hit San Francisco on April 18, 1906\n",
    "- Next Monday is Christmas!\n",
    "\n",
    "Can you do better? Play around with the model hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dbed0b30a94624aaa53e54b1903d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='sentence', layout=Layout(height='100px', width='auto'), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "layout = widgets.Layout(width='auto', height='100px') #set width and height\n",
    "\n",
    "# Custom Tokenizer\n",
    "re_tok = re.compile(f'([{string.punctuation}‚Äú‚Äù¬®¬´¬ª¬Æ¬¥¬∑¬∫¬Ω¬æ¬ø¬°¬ß¬£‚Ç§‚Äò‚Äô])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "    \n",
    "def get_prediction(sentence):\n",
    "    test_sentence = tokenize(sentence) # Tokenization\n",
    "    # Preprocessing\n",
    "    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=word2idx[\"PAD\"], maxlen=MAX_LEN)\n",
    "    # Evaluation\n",
    "    p = model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    # Visualization\n",
    "    print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "    print(30 * \"=\")\n",
    "    for w, pred in zip(test_sentence, p[0]):\n",
    "        print(\"{:15}: {:5}\".format(w, idx2tag[pred]))\n",
    "\n",
    "interact_manual(get_prediction, sentence=widgets.Textarea(placeholder='Type your sentence here', layout=layout));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
